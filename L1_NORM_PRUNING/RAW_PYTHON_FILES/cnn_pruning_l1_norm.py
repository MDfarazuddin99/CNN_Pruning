# -*- coding: utf-8 -*-
"""CNN_PRUNING_L1_NORM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ySSrkNIpb7D5Fh41ClEniM148C58H3A
"""

!pip install kerassurgeon

import numpy as np
import tensorflow as tf
from keras.datasets import cifar10
from keras.utils import np_utils
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, GlobalAveragePooling2D

from kerassurgeon import identify 
from kerassurgeon.operations import delete_channels,delete_layer

(x_train,y_train),(x_test,y_test) = cifar10.load_data()

print('Number of images in Test Set = {}\nNumber of images in Training Set = {}'.format(x_test.shape[0],x_train.shape[0]))
print('Number of classes in Data Set = {}'.format(np.unique(y_train).shape[0]))

print(y_train[2])
y_train = np_utils.to_categorical(y_train,10)
y_test = np_utils.to_categorical(y_test,10)
print(y_train[2])



import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()



# my_model = Sequential()

# my_model.add(Conv2D(32,(3,3),activation='relu',input_shape=x_train.shape[1:]))

# my_model.add(MaxPooling2D(pool_size=(2,2)))

# my_model.add(Conv2D(32,(3,3),activation='relu'))

# my_model.add(MaxPooling2D(pool_size=(2,2)))

# my_model.add(Conv2D(64,(3,3),activation='relu'))

# my_model.add(MaxPooling2D(pool_size=(2,2)))

# my_model.add(GlobalAveragePooling2D())
# my_model.add(Dense(10,activation='softmax'))

my_model = Sequential()
my_model.add(Conv2D(32, (3, 3), padding='same',activation='relu',
                 input_shape=x_train.shape[1:]))

my_model.add(Conv2D(32, (3, 3),activation='relu'))

my_model.add(MaxPooling2D(pool_size=(2, 2)))
my_model.add(Dropout(0.25))

my_model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))

my_model.add(Conv2D(64, (3, 3),activation='relu',))

my_model.add(MaxPooling2D(pool_size=(2, 2)))
my_model.add(Dropout(0.25))

my_model.add(Flatten())
my_model.add(Dense(512,activation='relu',))

my_model.add(Dropout(0.5))
my_model.add(Dense(10,activation='softmax',))

# my_model.summary()

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

my_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
my_model.summary()

x_train = x_train/255.
x_test = x_test/255.

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
my_model.fit(x_train, y_train, batch_size=32, epochs=1, verbose=1, validation_split=0.2, shuffle=True)

score_train = my_model.evaluate(x_train,y_train) 

print('Accuracy on the Train Images: ', score_train[1])

score_test = my_model.evaluate(x_test, y_test)

print('Accuracy on the Test Images: ', score_test[1])

all_conv_layers = list()
count = 0
for each_layer in (my_model.layers):
  # print(each_layer.name)
  if(each_layer.name[0:6] == 'conv2d'):
    all_conv_layers.append(count)
  count += 1
print(all_conv_layers)

weights = list()

for i in all_conv_layers:
  weights.append(my_model.layers[i].get_weights()[0])

layer_wise_filter_sorted = list()
layer_wise_filter_sorted_values = list()
for i in range(len(weights)):
  weight =  weights[i]
  num_filters = len(weight[0,0,0,:])
  print(num_filters)
  weights_dict = dict()
  for j in range(num_filters):
    w_s = np.sum(abs(weight[:,:,:,j]))
    filt = 'filt{}'.format(j)
    weights_dict[filt] = w_s

  weights_dict_sort = sorted(weights_dict.items(),key = lambda kv:kv[1])
  print('L1 norm of conv2D_{} layer'.format(i+1),weights_dict_sort)

  weight_values = list()
  filter_indices = list()

  for element in weights_dict_sort:
    filter_indices.append(int(element[0][4:]))
    weight_values.append(element[1])
  layer_wise_filter_sorted.append(filter_indices)
  layer_wise_filter_sorted_values.append(weight_values)
  x = np.arange(num_filters)
  plt.figure(i+1,figsize=(7,5))
  plt.plot(x,np.array(weight_values))
  plt.axhline(y=np.mean(np.array(weight_values)),c='r')
  for j in range(len(layer_wise_filter_sorted_values[i])):
    if(np.mean(np.array(weight_values)) < weight_values[j]):
        plt.axvline(x=j,c='r')
        break

  plt.xlabel('filter number')
  plt.ylabel('L1 norm')
  plt.title('Conv2d_{}'.format(i+1))
  plt.grid(True)
  plt.style.use(['classic'])
# print(np.array(weight_values),'\n',x) [22,24,2,29,23,13,0,14,26,21]
print(layer_wise_filter_sorted)
print(layer_wise_filter_sorted_values)

thresh_holds = list()
for i in range(len(all_conv_layers)):
  # print(np.mean(np.array(layer_wise_filter_sorted_values[i])))
  thresh_holds.append(np.mean(np.array(layer_wise_filter_sorted_values[i])))
prune_filter_indices = list()
for i in range(len(all_conv_layers)):
  for j in range(len(layer_wise_filter_sorted_values[i])):
    if(thresh_holds[i] < layer_wise_filter_sorted_values[i][j]):
      prune_filter_indices.append(j)
      break

prune_filter_indices

layer_indexes = []
pruning_layers = list()
layer_indexes.append(all_conv_layers[0])

for i in range(1,len(all_conv_layers)):
    layer_indexes.append(all_conv_layers[i])
print(layer_indexes)

for i in layer_indexes:
    pruning_layers.append(my_model.layers[i])

from kerassurgeon import Surgeon
surgeon = Surgeon(my_model)
# [0, 1, 4, 5]
for j,i in enumerate(all_conv_layers):
    surgeon.add_job('delete_channels',my_model.layers[i],channels=layer_wise_filter_sorted[j][0:prune_filter_indices[j]])

model_new = surgeon.operate()

model_new.summary()

model_new.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
score_train = model_new.evaluate(x_train,y_train) 

print('Accuracy on the Train Images: ', score_train[1])

score_test = model_new.evaluate(x_test, y_test)

print('Accuracy on the Test Images: ', score_test[1])

model_new.fit(x_train, y_train, batch_size=32, epochs=1, verbose=1, validation_split=0.2, shuffle=True)

score_train = model_new.evaluate(x_train,y_train) 

print('Accuracy on the Train Images: ', score_train[1])

score_test = model_new.evaluate(x_test, y_test)

print('Accuracy on the Test Images: ', score_test[1])

