# -*- coding: utf-8 -*-
"""Untitled2.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1k6R1XJeAMUXO5kkYw0Bt4cCpN-d6L9_-
"""

# Commented out IPython magic to ensure Python compatibility.
#change
# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
# !pip install tesnsorflow 1.x
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

import keras
from keras.datasets import cifar10
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, GlobalAveragePooling2D,BatchNormalization,Activation
from keras.models import load_model
from keras.callbacks import Callback
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.layers.core import Lambda
from keras import backend as K
from keras import regularizers
from keras.callbacks import ModelCheckpoint



# !pip install kerassurgeon
from kerassurgeon import identify 
from kerassurgeon.operations import delete_channels,delete_layer
from kerassurgeon import Surgeon

def my_get_all_conv_layers(model , first_time):

    '''
    Arguments:
        model -> your model
        first_time -> type boolean 
            first_time = True => model is not pruned 
            first_time = False => model is pruned
    Return:
        List of Indices containing convolution layers
    '''

    all_conv_layers = list()
    for i,each_layer in enumerate(model.layers):
        if (each_layer.name[0:6] == 'conv2d'):
            all_conv_layers.append(i)
    return all_conv_layers if (first_time==True) else all_conv_layers[1:]


def my_get_all_dense_layers(model):
    '''
    Arguments:
        model -> your model        
    Return:
        List of Indices containing fully connected layers
    '''
    all_dense_layers = list()
    for i,each_layer in enumerate(model.layers):
        if (each_layer.name[0:5] == 'dense'):
            all_dense_layers.append(i)
    return all_dense_layers




def count_conv_params_flops(conv_layer):
    # out shape is  n_cells_dim1 * (n_cells_dim2 * n_cells_dim3)
    '''
    Arguments:
        conv layer 
    Return:
        Number of Parameters, Number of Flops
    '''
    
    
    out_shape = conv_layer.output.shape.as_list()
    n_cells_total = np.prod(out_shape[1:-1])

    n_conv_params_total = conv_layer.count_params()

    conv_flops = n_conv_params_total * n_cells_total

 

    return n_conv_params_total, conv_flops


def count_dense_params_flops(dense_layer):
    # out shape is  n_cells_dim1 * (n_cells_dim2 * n_cells_dim3)
    '''
    Arguments:
      dense layer 
    Return:
        Number of Parameters, Number of Flops
    '''

    out_shape = dense_layer.output.shape.as_list()
    n_cells_total = np.prod(out_shape[1:-1])

    n_dense_params_total = dense_layer.count_params()

    dense_flops = n_dense_params_total


    return n_dense_params_total, dense_flops




def count_model_params_flops(model,first_time):

    '''
    Arguments:
        model -> your model
        first_time -> boolean variable
        first_time = True => model is not pruned 
        first_time = False => model is pruned
    Return:
        Number of parmaters, Number of Flops
    '''

    total_params = 0
    total_flops = 0
    # if first_time == True:
    #     model_layers = model.layers
    # else:
    #     model_layers = model.layers[1:-2]
    # all_conv_layers = my_get_all_conv_layers(model,first_time)
    # all_dense_layers = my_get_all_dense_layers(model)
    # model_layers = list()
    
    # for i in all_conv_layers:
    #     model_layers.append(model.layers[i])
    # for i in all_dense_layers:
    #     model_layers.append(model.layers[i])
    model_layers = model.layers[1:-2]
    for index,layer in enumerate(model_layers):
        if any(conv_type in str(type(layer)) for conv_type in ['Conv1D', 'Conv2D', 'Conv3D']):
            print(index,layer.name)
            params, flops = count_conv_params_flops(layer)
            total_params += params
            total_flops += flops
        elif 'Dense' in str(type(layer)):
            print(index,layer.name) 
            params, flops = count_dense_params_flops(layer)
            total_params += params
            total_flops += flops
    return total_params, total_flops

def my_get_weights_in_conv_layers(model,first_time):

    '''
    Arguments:
        model -> your model
        first_time -> boolean variable
            first_time = True => model is not pruned 
            first_time = False => model is pruned
    Return:
        List containing weight tensors of each layer
    '''
    


    weights = list()
    all_conv_layers = my_get_all_conv_layers(model,first_time)
    layer_wise_weights = list() 
    for i in all_conv_layers:
          weights.append(model.layers[i].get_weights()[0])  
    return weights

def my_get_l1_norms_filters_per_epoch(weight_list_per_epoch):

    '''
    Arguments:
        List
    Return:
        Number of parmaters, Number of Flops
    '''
    
    # weight_list_per_epoch = my_get_weights_in_conv_layers(model,first_time)
    l1_norms_filters_per_epoch = list()
    

    for index in range(len(weight_list_per_epoch)):

        epochs = np.array(weight_list_per_epoch[index]).shape[0]
        h , w , d = np.array(weight_list_per_epoch[index]).shape[1], np.array(weight_list_per_epoch[index]).shape[2] , np.array(weight_list_per_epoch[index]).shape[3]

        a =np.abs (np.array(weight_list_per_epoch[index]))
        l1_norms_filters_per_epoch.append(np.sum(a.reshape(epochs,h*w*d,-1),axis=1))
    return l1_norms_filters_per_epoch

def my_in_conv_layers_get_sum_of_l1_norms_sorted_indices(weight_list_per_epoch):
    layer_wise_filter_sorted_indices = list()
    layer_wise_filter_sorted_values = list()
    l1_norms_filters_per_epoch = my_get_l1_norms_filters_per_epoch(weight_list_per_epoch)
    sum_l1_norms = list()
    
    for i in l1_norms_filters_per_epoch:
        sum_l1_norms.append(np.sum(i,axis=0))
    
    layer_wise_filter_sorted_indices = list()
    
    for i in sum_l1_norms:
        a = pd.Series(i).sort_values().index
        layer_wise_filter_sorted_indices.append(a.tolist())
    return layer_wise_filter_sorted_indices


def my_get_percent_prune_filter_indices(layer_wise_filter_sorted_indices,percentage):    

    prune_filter_indices = list()
    for i in range(len(layer_wise_filter_sorted_indices)):
        prune_filter_indices.append(int(len(layer_wise_filter_sorted_indices[i]) * (percentage/100))   )
    return prune_filter_indices



def my_delete_filters(model,weight_list_per_epoch,percentage,first_time):

    sum_of_l1_norms_sorted_indices = my_in_conv_layers_get_sum_of_l1_norms_sorted_indices(weight_list_per_epoch)

    layer_wise_filter_sorted_indices = my_in_conv_layers_get_sum_of_l1_norms_sorted_indices(weight_list_per_epoch)
    # print(layer_wise_filter_sorted_indices)
    prune_filter_indices = my_get_percent_prune_filter_indices(layer_wise_filter_sorted_indices,percentage)
    # print(prune_filter_indices)
    all_conv_layers = my_get_all_conv_layers(model,first_time)

    surgeon = Surgeon(model)
    for index,value in enumerate(all_conv_layers):
        # print(index,value,layer_wise_filter_sorted_indices[index][0:prune_filter_indices[index]])
        surgeon.add_job('delete_channels',model.layers[value],channels = layer_wise_filter_sorted_indices[index][0:prune_filter_indices[index]])

    model_new = surgeon.operate()
 
    
    return model_new

class Get_Weights(Callback):
    def __init__(self,first_time):
        super(Get_Weights, self).__init__()
        self.weight_list = [] #Using a list of list to store weight tensors per epoch
        self.first_time = first_time
    def on_epoch_end(self,epoch,logs=None):
        if epoch == 0:
            all_conv_layers = my_get_all_conv_layers(self.model,self.first_time)
            for i in range(len(all_conv_layers)):
                self.weight_list.append([]) # appending empty lists for later appending weight tensors 
        
        for index,each_weight in enumerate(my_get_weights_in_conv_layers(self.model,self.first_time)):
                self.weight_list[index].append(each_weight)  



class cifar10vgg:

    def __init__(self,first_time,epochs,train=True):
        self.epochs = epochs
        self.first_time = first_time
        self.num_classes = 10
        self.weight_decay = 0.0005
        self.x_shape = [32,32,3]
        self.history = 0
        self.weight_list_per_epoch = None
        self.model = self.build_model()
        if train:
            self.model, self.history ,self.weight_list_per_epoch = self.train(self.model)
        else:
            #change

            self.model.load_weights('/home/shabbeer/Research/BTP_Pruning/best_vgg16_cifar10.h5')


    def build_model(self):
        # Build the network of vgg for 10 classes with massive dropout and weight decay as described in the paper.

        model = Sequential()
        weight_decay = self.weight_decay

        model.add(Conv2D(64, (3, 3), padding='same',
                         input_shape=self.x_shape,kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.3))

        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())

        model.add(MaxPooling2D(pool_size=(2, 2)))

        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))

        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())

        model.add(MaxPooling2D(pool_size=(2, 2)))

        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))

        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))

        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())

        model.add(MaxPooling2D(pool_size=(2, 2)))


        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))

        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))

        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())

        model.add(MaxPooling2D(pool_size=(2, 2)))


        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))

        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))

        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())

        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.5))

        model.add(Flatten())
        model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))
        model.add(Activation('relu'))
        model.add(BatchNormalization())

        model.add(Dropout(0.5))
        model.add(Dense(self.num_classes))
        model.add(Activation('softmax'))
        return model


    def normalize(self,X_train,X_test):
        #this function normalize inputs for zero mean and unit variance
        # it is used when training a model.
        # Input: training set and test set
        # Output: normalized training set and test set according to the trianing set statistics.
        mean = np.mean(X_train,axis=(0,1,2,3))
        std = np.std(X_train, axis=(0, 1, 2, 3))
        X_train = (X_train-mean)/(std+1e-7)
        X_test = (X_test-mean)/(std+1e-7)
        return X_train, X_test


    def train(self,model):

        #training parameters
        batch_size = 128
        maxepoches = 10
        learning_rate = 0.1
        lr_decay = 1e-6
        lr_drop = 20
        # The data, shuffled and split between train and test sets:
        (x_train, y_train), (x_test, y_test) = cifar10.load_data()
        x_train = x_train.astype('float32')
        x_test = x_test.astype('float32')
        x_train, x_test = self.normalize(x_train, x_test)

        y_train = keras.utils.to_categorical(y_train, self.num_classes)
        y_test = keras.utils.to_categorical(y_test, self.num_classes)

        def lr_scheduler(epoch):
            return learning_rate * (0.5 ** (epoch // lr_drop))
        reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)



        #data augmentation
        datagen = ImageDataGenerator(
            featurewise_center=False,  # set input mean to 0 over the dataset
            samplewise_center=False,  # set each sample mean to 0
            featurewise_std_normalization=False,  # divide inputs by std of the dataset
            samplewise_std_normalization=False,  # divide each input by its std
            zca_whitening=False,  # apply ZCA whitening
            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
            horizontal_flip=True,  # randomly flip images
            vertical_flip=False)  # randomly flip images
        # (std, mean, and principal components if ZCA whitening is applied).
        # datagen.fit(x_train)



        #optimization details
        sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)
        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])

        gw = Get_Weights(self.first_time)

        # training process in a for loop with learning rate drop every 25 epoches.

        history = model.fit_generator(datagen.flow(x_train, y_train,
                                         batch_size=batch_size),
                            steps_per_epoch=x_train.shape[0] // batch_size,
                            epochs=self.epochs,
                            validation_data=(x_test, y_test),callbacks=[reduce_lr,gw],verbose=1)

        return model, history,gw.weight_list

def normalize(X_train,X_test):
    #this function normalize inputs for zero mean and unit variance
    # it is used when training a model.
    # Input: training set and test set
    # Output: normalized training set and test set according to the trianing set statistics.
    mean = np.mean(X_train,axis=(0,1,2,3))
    std = np.std(X_train, axis=(0, 1, 2, 3))
    X_train = (X_train-mean)/(std+1e-7)
    X_test = (X_test-mean)/(std+1e-7)
    return X_train, X_test


def train(model,epochs):

    #training parameters
    batch_size = 128
    learning_rate = 0.01
    lr_decay = 1e-6
    lr_drop = 20

    num_classes = 10
    weight_decay = 0.0005
    x_shape = [32,32,3]

    # The data, shuffled and split between train and test sets:
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train, x_test = normalize(x_train, x_test)

    y_train = keras.utils.to_categorical(y_train, num_classes)
    y_test = keras.utils.to_categorical(y_test, num_classes)

    def lr_scheduler(epoch):
        return learning_rate * (0.5 ** (epoch // lr_drop))
    reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)


    #data augmentation
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False)  # randomly flip images
    # (std, mean, and principal components if ZCA whitening is applied).
    # datagen.fit(x_train)



    #optimization details
    sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])

    gw = Get_Weights(False)

    # training process in a for loop with learning rate drop every 25 epoches.

    history = model.fit_generator(datagen.flow(x_train, y_train,
                                        batch_size=batch_size),
                        steps_per_epoch=x_train.shape[0] // batch_size,
                        epochs=epochs,
                        validation_data=(x_test, y_test),callbacks=[reduce_lr,gw],verbose=1)

    return model, history,gw.weight_list

#this dictionary is to log the parameters and is later converted into a dataframe.
log_dict = dict()
log_dict['train_loss'] = []
log_dict['train_acc'] = []
log_dict['val_loss'] = []
log_dict['val_acc'] = []
log_dict['total_params'] = []
log_dict['total_flops'] = []





# train for first time
# choice = input('Use pretrained model [Y/N]: ')
# if choice == 'Y':
#     # Use the best pretrained model to avoid training from scratch
#     my_vgg = cifar10vgg(first_time=True,epochs=5,train=False)
#     model = my_vgg.model
#     (x_train,y_train),(x_test,y_test) = cifar10.load_data()
#     x_train,x_test = normalize(x_train,x_test)
#     y_train = keras.utils.to_categorical(y_train, 10)
#     y_test = keras.utils.to_categorical(y_test, 10)    

#     def lr_scheduler(epoch):
#         return learning_rate * (0.5 ** (epoch // lr_drop))
#     reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)
#     sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
#     model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])
#     validation_accuracy = model.evaluate(x_test,y_test)[1]
#     validation_loss = model.evaluate(x_test,y_test)[0]
#     train_accuracy = model.evaluate(x_train,y_train)[1]
#     train_loss = model.evaluate(x_test,y_test)[0]
#     log_dict['train_loss'].append(train_loss)
#     log_dict['train_acc'].append(train_accuracy)
#     log_dict['val_loss'].append(validation_loss)
#     log_dict['val_acc'].append(validation_accuracy)
#     a,b = count_model_params_flops(model,True)
#     log_dict['total_params'].append(a)
#     log_dict['total_flops'].append(b)

# else:
#     # Train from scratch and save the results
# my_vgg = cifar10vgg(first_time=True,epochs=200)
# model, history ,weight_list_per_epoch= my_vgg.model, my_vgg.history, my_vgg.weight_list_per_epoch
# model.save('/home/shabbeer/Research/BTP_Pruning/trained_cifarvgg.h5')
# best_acc_index = history.history['val_acc'].index(max(history.history['val_acc']))
# log_dict['train_loss'].append(history.history['loss'][best_acc_index])
# log_dict['train_acc'].append(history.history['acc'][best_acc_index])
# log_dict['val_loss'].append(history.history['val_loss'][best_acc_index])
# log_dict['val_acc'].append(history.history['val_acc'][best_acc_index])
# a,b = count_model_params_flops(model,True)
# log_dict['total_params'].append(a)
# log_dict['total_flops'].append(b)
# validation_accuracy = max(history.history['val_acc'])

choice = input("USE PRETRAINED VGG16 FOR CIFAR10 [Y/N] : ")
if choice == 'Y':
    my_vgg = cifar10vgg(first_time=True,epochs=0,train=False)
    model = my_vgg.model
    learning_rate = 0.1
    lr_decay = 1e-6
    lr_drop = 20
    
    def lr_scheduler(epoch):
        return learning_rate * (0.5 ** (epoch // lr_drop))
    reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)

    sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])

    weight_list_per_epoch = list()
    data = np.load("/home/shabbeer/Research/BTP_Pruning/1.npz")
    for i in range(13):
        weight_list_per_epoch.append(data['w_{}'.format(i+1)])

    (x_train,y_train),(x_test,y_test) = cifar10.load_data()
    x_train,x_test = normalize(x_train,x_test)
    y_train = keras.utils.to_categorical(y_train,10)
    y_test = keras.utils.to_categorical(y_test,10)
    train_loss,train_acc = model.evaluate(x_train,y_train)
    val_loss,val_acc = model.evaluate(x_test,y_test)
    validation_accuracy = val_acc
    log_dict['train_loss'].append(train_loss)
    log_dict['train_acc'].append(train_acc)
    log_dict['val_loss'].append(val_loss)
    log_dict['val_acc'].append(val_acc)

elif choice == 'N':
    # train for first time
    my_vgg = cifar10vgg(first_time=True,epochs=250)
    model, history ,weight_list_per_epoch= my_vgg.model, my_vgg.history, my_vgg.weight_list_per_epoch

    model.save_weights('/home/shabbeer/Research/BTP_Pruning/best_vgg16_cifar10.h5')
    #save the weights of training process
    np.savez("/home/shabbeer/Research/BTP_Pruning/1.npz"
            ,w_1=weight_list_per_epoch[0],
            w_2=weight_list_per_epoch[1],
            w_3=weight_list_per_epoch[2],
            w_4=weight_list_per_epoch[3],
            w_5=weight_list_per_epoch[4],
            w_6=weight_list_per_epoch[5],
            w_7=weight_list_per_epoch[6],
            w_8=weight_list_per_epoch[7],
            w_9=weight_list_per_epoch[8],
            w_10=weight_list_per_epoch[9],
            w_11=weight_list_per_epoch[10],
            w_12=weight_list_per_epoch[11],
            w_13=weight_list_per_epoch[12])
    best_acc_index = history.history['val_acc'].index(max(history.history['val_acc']))
    log_dict['train_loss'].append(history.history['loss'][best_acc_index])
    log_dict['train_acc'].append(history.history['acc'][best_acc_index])
    log_dict['val_loss'].append(history.history['val_loss'][best_acc_index])
    log_dict['val_acc'].append(history.history['val_acc'][best_acc_index])
    validation_accuracy = max(history.history['val_acc'])

a,b = count_model_params_flops(model,True)
log_dict['total_params'].append(a)
log_dict['total_flops'].append(b)

# model.summary()

#stop pruning if the accuracy drops by 5% from maximum accuracy ever obtained. 
print('Initial Validation Accuracy {}'.format(validation_accuracy))
max_val_acc = validation_accuracy
count = 0
all_models = list()
while validation_accuracy - max_val_acc >= -0.015 :

# while max_val_acc <= validation_accuracy  :

    print("ITERATION {} ".format(count+1))
    all_models.append(model)
    if max_val_acc < validation_accuracy:
        max_val_acc = validation_accuracy
        

    if count < 1:
        model = my_delete_filters(model,weight_list_per_epoch,10,True)

   
    else:
        model = my_delete_filters(model,weight_list_per_epoch,10,False)
    
    
    a,b = count_model_params_flops(model,False)
    model,history,weight_list_per_epoch = train(model,150)

    validation_accuracy = max(history.history['val_acc'])
    best_acc_index = history.history['val_acc'].index(max(history.history['val_acc']))
    log_dict['train_loss'].append(history.history['loss'][best_acc_index])
    log_dict['train_acc'].append(history.history['acc'][best_acc_index])
    log_dict['val_loss'].append(history.history['val_loss'][best_acc_index])
    log_dict['val_acc'].append(history.history['val_acc'][best_acc_index])
    log_dict['total_params'].append(a)
    log_dict['total_flops'].append(b)

    print("VALIDATION ACCURACY AFTER {} ITERATIONS = {}".format(count+1,validation_accuracy))
    count+=1



#sir, I think after this we should train for more number of epochs
#change 
# print('PRUNING COMPLETED')
model,history,weight_list_per_epoch = train(model,50)
validation_accuracy = max(history.history['val_acc'])
print('Final Validation Accuracy {}'.format(validation_accuracy))
best_acc_index = history.history['val_acc'].index(max(history.history['val_acc']))
log_dict['train_loss'].append(history.history['loss'][best_acc_index])
log_dict['train_acc'].append(history.history['acc'][best_acc_index])
log_dict['val_loss'].append(history.history['val_loss'][best_acc_index])
log_dict['val_acc'].append(history.history['val_acc'][best_acc_index])
model = my_delete_filters(model,weight_list_per_epoch,10,False)
log_dict['total_params'].append(a)
log_dict['total_flops'].append(b)

log_df = pd.DataFrame(log_dict)
#change
log_df.to_csv('/home/shabbeer/Research/BTP_Pruning/EXP_10_RESULT_VGG_HISTORY_L1_PRUNING.csv')
